---
title: "STAT507-CS2-UFC"
author: "Aaron Pongsugree, Isaac Amouzou"
format: html
editor: visual
---

# UFC Historical Data Statistical Analyses

## Data Cleaning

Clean data, handle missing values, and create exploratory visualizations

Overview of the Dataset

```{r}
ufc_data <- read.csv("Data/ufcdata.csv", stringsAsFactors = FALSE)

str(ufc_data)
summary(ufc_data)
dim(ufc_data)
names(ufc_data)

```

### Identify the missing data points

```{r, warning= FALSE}
library(tidyverse)

#Count missing values per column
missing_count <- colSums(is.na(ufc_data))

#Calculate percentage of missing values
missing_percent <- (missing_count / nrow(ufc_data)) * 100

#Create a data frame of columns with missing values
missing_data <- data.frame(
  column = names(ufc_data),
  count = missing_count,
  percent = missing_percent
) %>%
  filter(count > 0) %>% 
  arrange(desc(percent))

print(missing_data)

#Identify columns with high missingness (>20%)
high_missing <- missing_data %>% filter(percent > 20)
print("Columns with >20% missing values:")
print(high_missing)

#Check if entire rows are missing
rows_missing_count <- rowSums(is.na(ufc_data))
rows_with_any_missing <- sum(rows_missing_count > 0)
cat("Number of rows with at least one missing value:", rows_with_any_missing, 
    "(", round(rows_with_any_missing/nrow(ufc_data)*100, 2), "%)\n")

#informative visualization for missing data
ggplot(missing_data, aes(x = reorder(column, -percent), y = percent)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(), 
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    panel.grid.minor = element_blank() # Remove minor grid lines
  ) +
  labs(
    title = "Missing Data in UFC Dataset",
    subtitle = "Blue corner statistics are more than twice as likely to be missing compared to red corner",
    x = "",
    y = "Percent Missing (%)"
  ) +
  scale_y_continuous(
    limits = c(0, 30),
    breaks = seq(0, 30, by = 5),
    expand = c(0, 0)
  ) +
  annotate("text", x = 25, y = 25, label = "Blue corner fight statistics (~24%)", 
           vjust = -0.5, hjust = 0.5, fontface = "bold", color = "darkblue") +
  annotate("text", x = 75, y = 13, label = "Red corner fight statistics (~12%)", 
           vjust = -0.5, hjust = 0.5, fontface = "bold", color = "darkred") +
  annotate("text", x = 50, y = 3, 
           label = "Physical attributes (height, reach, weight) have much lower missingness (<3%)",
           vjust = -1, hjust = 0.5, size = 3.5) +
  # Add a note about the impact at the top of the plot
  annotate("text", x = 50, y = 29, 
           label = "34.85% of all observations have at least one missing value",
           hjust = 0.5, fontface = "italic", size = 3.5)

```

### pattern of missingness

Temporal Pattern Analysis: Is missingness more common in older fights?

```{r, warning= FALSE}
ufc_data$date <- as.Date(ufc_data$date)
ufc_data$year <- format(ufc_data$date, "%Y")

#Calculate percentage of missing values by year
missing_by_year <- ufc_data %>%
  group_by(year) %>%
  summarize(
    total_fights = n(),
    missing_blue_stats = sum(is.na(B_avg_KD)),
    missing_red_stats = sum(is.na(R_avg_KD)),
    pct_missing_blue = missing_blue_stats / total_fights * 100,
    pct_missing_red = missing_red_stats / total_fights * 100
  )

#Reshape data for plotting
missing_by_year_long <- missing_by_year %>%
  select(year, pct_missing_blue, pct_missing_red) %>%
  pivot_longer(
    cols = c(pct_missing_blue, pct_missing_red),
    names_to = "corner",
    values_to = "percentage"
  ) %>%
  mutate(corner = ifelse(corner == "pct_missing_blue", "Blue Corner", "Red Corner"))

#Visualize with proper grouping
ggplot(missing_by_year_long, aes(x = year, y = percentage, color = corner, group = corner)) +
  geom_line() +
  geom_point() +
  labs(title = "Missing Values by Year",
       y = "Percentage Missing", 
       x = "Year",
       color = "Fighter Corner") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Cleaning the missing data values using

```{r, warning= FALSE}
library(tidyverse)

ufc_data <- read.csv("data/ufcdata.csv", stringsAsFactors = FALSE)

ufc_data$date <- as.Date(ufc_data$date)

#only post-2010 data
ufc_data_post_2010 <- ufc_data %>%
  filter(date >= as.Date("2010-01-01"))

ufc_clean <- ufc_data_post_2010[complete.cases(ufc_data_post_2010), ]

#Check how many rows were retained
original_rows <- nrow(ufc_data)
post_2010_rows <- nrow(ufc_data_post_2010)
clean_rows <- nrow(ufc_clean)
retention_rate_original <- (clean_rows / original_rows) * 100
retention_rate_post_2010 <- (clean_rows / post_2010_rows) * 100

#Print summary
cat("Original dataset:", original_rows, "rows\n")
cat("Post-2010 dataset:", post_2010_rows, "rows\n")
cat("Clean post-2010 dataset:", clean_rows, "rows\n")
cat("Retention rate from original:", round(retention_rate_original, 2), "%\n")
cat("Retention rate from post-2010 only:", round(retention_rate_post_2010, 2), "%\n")

write.csv(ufc_clean, "data/ufc_clean.csv", row.names = FALSE)
```

Feature engineering complete case

```{r, warning= FALSE}

library(tidyverse)

ufc_clean_feature <- read.csv("data/ufc_clean.csv", stringsAsFactors = FALSE)

#Physical Advantage Metrics
ufc_clean_feature$height_advantage <- ufc_clean_feature$R_Height_cms - ufc_clean_feature$B_Height_cms
ufc_clean_feature$reach_advantage <- ufc_clean_feature$R_Reach_cms - ufc_clean_feature$B_Reach_cms
ufc_clean_feature$age_advantage <- ufc_clean_feature$R_age - ufc_clean_feature$B_age
ufc_clean_feature$weight_advantage <- ufc_clean_feature$R_Weight_lbs - ufc_clean_feature$B_Weight_lbs

#Win Percentage and Experience Features
ufc_clean_feature$R_win_pct <- ifelse(ufc_clean_feature$R_wins + ufc_clean_feature$R_losses > 0, 
                              ufc_clean_feature$R_wins / (ufc_clean_feature$R_wins + ufc_clean_feature$R_losses), 
                              0.5)
ufc_clean_feature$B_win_pct <- ifelse(ufc_clean_feature$B_wins + ufc_clean_feature$B_losses > 0, 
                              ufc_clean_feature$B_wins / (ufc_clean_feature$B_wins + ufc_clean_feature$B_losses), 
                              0.5)
ufc_clean_feature$win_pct_advantage <- ufc_clean_feature$R_win_pct - ufc_clean_feature$B_win_pct
ufc_clean_feature$experience_diff <- ufc_clean_feature$R_total_rounds_fought - ufc_clean_feature$B_total_rounds_fought

#Performance Differential Metrics
#Striking metrics
ufc_clean_feature$striking_accuracy_diff <- ufc_clean_feature$R_avg_SIG_STR_pct - ufc_clean_feature$B_avg_SIG_STR_pct
ufc_clean_feature$striking_defense_diff <- (1-ufc_clean_feature$R_avg_opp_SIG_STR_pct) - (1-ufc_clean_feature$B_avg_opp_SIG_STR_pct)

# Grappling metrics
ufc_clean_feature$takedown_accuracy_diff <- ufc_clean_feature$R_avg_TD_pct - ufc_clean_feature$B_avg_TD_pct
ufc_clean_feature$takedown_defense_diff <- (1-ufc_clean_feature$R_avg_opp_TD_pct) - (1-ufc_clean_feature$B_avg_opp_TD_pct)
ufc_clean_feature$submission_attempts_diff <- ufc_clean_feature$R_avg_SUB_ATT - ufc_clean_feature$B_avg_SUB_ATT

#Impact metrics
ufc_clean_feature$knockdown_diff <- ufc_clean_feature$R_avg_KD - ufc_clean_feature$B_avg_KD

#Style Indicator Features
ufc_clean_feature$R_striking_to_grappling_ratio <- ifelse(ufc_clean_feature$R_avg_TD_att > 0, 
                                                 ufc_clean_feature$R_avg_SIG_STR_att / ufc_clean_feature$R_avg_TD_att, 
                                                 ufc_clean_feature$R_avg_SIG_STR_att) 
ufc_clean_feature$B_striking_to_grappling_ratio <- ifelse(ufc_clean_feature$B_avg_TD_att > 0, 
                                                 ufc_clean_feature$B_avg_SIG_STR_att / ufc_clean_feature$B_avg_TD_att, 
                                                 ufc_clean_feature$B_avg_SIG_STR_att)

#Calculate offensive vs. defensive balance
ufc_clean_feature$R_offense_defense_ratio <- ufc_clean_feature$R_avg_SIG_STR_landed / (ufc_clean_feature$R_avg_opp_SIG_STR_landed + 0.1)
ufc_clean_feature$B_offense_defense_ratio <- ufc_clean_feature$B_avg_SIG_STR_landed / (ufc_clean_feature$B_avg_opp_SIG_STR_landed + 0.1)
ufc_clean_feature$offense_defense_diff <- ufc_clean_feature$R_offense_defense_ratio - ufc_clean_feature$B_offense_defense_ratio

#Create categorical style variables
ufc_clean_feature$R_style <- case_when(
  ufc_clean_feature$R_striking_to_grappling_ratio > 20 ~ "Striker",
  ufc_clean_feature$R_striking_to_grappling_ratio < 10 ~ "Grappler",
  TRUE ~ "Balanced"
)
ufc_clean_feature$B_style <- case_when(
  ufc_clean_feature$B_striking_to_grappling_ratio > 20 ~ "Striker",
  ufc_clean_feature$B_striking_to_grappling_ratio < 10 ~ "Grappler",
  TRUE ~ "Balanced"
)
ufc_clean_feature$R_approach <- case_when(
  ufc_clean_feature$R_offense_defense_ratio > 1.5 ~ "Offensive",
  ufc_clean_feature$R_offense_defense_ratio < 0.7 ~ "Defensive",
  TRUE ~ "Balanced"
)
ufc_clean_feature$B_approach <- case_when(
  ufc_clean_feature$B_offense_defense_ratio > 1.5 ~ "Offensive",
  ufc_clean_feature$B_offense_defense_ratio < 0.7 ~ "Defensive",
  TRUE ~ "Balanced"
)

ufc_clean_feature$target <- ifelse(ufc_clean_feature$Winner == "Red", 1, 0)

write.csv(ufc_clean_feature, "data/ufc_clean_features_effects.csv", row.names = FALSE)

```

implement mixed-effect models with fighter IDs as random effects to address data dependency issues for CCA

```{r, warning = FALSE}

library(tidyverse)
library(lme4)     


ufc_model_data <- read.csv("data/ufc_clean_features.csv", stringsAsFactors = FALSE)

#Convert date to Date type for temporal analysis
ufc_model_data$date <- as.Date(ufc_model_data$date)

#Extract unique fighters and assign IDs
all_fighters <- unique(c(ufc_model_data$R_fighter, ufc_model_data$B_fighter))
fighter_ids <- data.frame(
  fighter_name = all_fighters,
  fighter_id = 1:length(all_fighters),
  stringsAsFactors = FALSE
)

#Map fighter IDs back to the dataset
ufc_model_data <- ufc_model_data %>%
  left_join(fighter_ids, by = c("R_fighter" = "fighter_name")) %>%
  rename(R_fighter_id = fighter_id) %>%
  left_join(fighter_ids, by = c("B_fighter" = "fighter_name")) %>%
  rename(B_fighter_id = fighter_id)

#select key predictors
key_predictors <- c(
  #Physical advantages
  "age_advantage", "reach_advantage", 
  
  #Performance differentials
  "striking_accuracy_diff", "striking_defense_diff",
  "takedown_accuracy_diff", "takedown_defense_diff", 
  "knockdown_diff",
  
  #Experience and record
  "win_pct_advantage", "R_longest_win_streak", "experience_diff"
)

#Prepare model dataset
model_data <- ufc_model_data %>%
  select(R_fighter_id, B_fighter_id, all_of(key_predictors), target) %>%
  mutate(target = as.factor(target))

#Fit mixed-effects model
mixed_model <- glmer(
  target ~ age_advantage + reach_advantage + 
           striking_accuracy_diff + striking_defense_diff +
           takedown_accuracy_diff + takedown_defense_diff +
           knockdown_diff + win_pct_advantage +
           (1|R_fighter_id) + (1|B_fighter_id),
  data = model_data,
  family = binomial,
  control = glmerControl(optimizer = "bobyqa")
)

summary_mixed <- summary(mixed_model)
print(summary_mixed)

#Extract fighter-specific random effects
red_fighter_effects <- ranef(mixed_model)$R_fighter_id
blue_fighter_effects <- ranef(mixed_model)$B_fighter_id

#Create dataframes with fighter IDs and their random effects
red_fighter_df <- data.frame(
  R_fighter_id = as.integer(rownames(red_fighter_effects)),
  R_fighter_effect = red_fighter_effects[,1]
)

blue_fighter_df <- data.frame(
  B_fighter_id = as.integer(rownames(blue_fighter_effects)),
  B_fighter_effect = blue_fighter_effects[,1]
)

#Add fighter random effects to the original dataset
ufc_model_data_with_effects <- ufc_model_data %>%
  left_join(red_fighter_df, by = "R_fighter_id") %>%
  left_join(blue_fighter_df, by = "B_fighter_id") %>%
  mutate(
    R_fighter_effect = ifelse(is.na(R_fighter_effect), 0, R_fighter_effect),
    B_fighter_effect = ifelse(is.na(B_fighter_effect), 0, B_fighter_effect)
  )

#Create a new feature that represents the difference in fighter-specific effects
ufc_model_data_with_effects$fighter_effect_diff <- 
  ufc_model_data_with_effects$R_fighter_effect - ufc_model_data_with_effects$B_fighter_effect



print(summary(red_fighter_df$R_fighter_effect))


print(summary(blue_fighter_df$B_fighter_effect))


print(summary(ufc_model_data_with_effects$fighter_effect_diff))

write.csv(ufc_model_data_with_effects, 
          "data/ufc_clean_features_effects.csv", 
          row.names = FALSE)

```

Visualization for feature engineering and mixed-effect models:

```{r, warning = FALSE}
library(ggplot2)

fighter_effects_df <- data.frame(
  Effect = c(red_fighter_df$R_fighter_effect, blue_fighter_df$B_fighter_effect),
  Corner = c(rep("Red Corner", length(red_fighter_df$R_fighter_effect)), 
             rep("Blue Corner", length(blue_fighter_df$B_fighter_effect)))
)

# Create boxplot
ggplot(fighter_effects_df, aes(x = Corner, y = Effect, fill = Corner)) +
  geom_boxplot() +
  labs(title = "Fighter-Specific Random Effects by Corner",
       subtitle = paste0("Red Corner SD: ", round(sd(red_fighter_df$R_fighter_effect), 4), 
                        ", Blue Corner SD: ", round(sd(blue_fighter_df$B_fighter_effect), 4)),
       y = "Random Effect Size",
       x = "") +
  scale_fill_manual(values = c("Blue Corner" = "blue", "Red Corner" = "red")) +
  theme_minimal() +
  theme(legend.position = "none")
```

logistic regression model for clean data using complete case

```{r, warning= FALSE}

library(tidyverse)
library(glmnet)   
library(caret)      
library(pROC)        

ufc_clean_features_effects <- read.csv("data/ufc_clean_features_effects.csv", stringsAsFactors = FALSE)

#Create list of model features
model_features <- c(
  #Physical attributes
  "height_advantage", "reach_advantage", "age_advantage", "weight_advantage",
  
  #Experience and record
  "win_pct_advantage", "experience_diff",
  "R_current_win_streak", "B_current_win_streak", 
  "R_longest_win_streak", "B_longest_win_streak",
  
  #Fighting metrics - differentials
  "striking_accuracy_diff", "striking_defense_diff", 
  "takedown_accuracy_diff", "takedown_defense_diff", 
  "knockdown_diff", "submission_attempts_diff", "offense_defense_diff",
  
  #Raw metrics - both fighters
  "R_avg_SIG_STR_pct", "B_avg_SIG_STR_pct",
  "R_avg_TD_pct", "B_avg_TD_pct",
  "R_avg_SUB_ATT", "B_avg_SUB_ATT",
  "R_avg_KD", "B_avg_KD",
  
  #Style indicators
  "R_striking_to_grappling_ratio", "B_striking_to_grappling_ratio"
)

#Create the model dataset with selected features and target
model_data <- ufc_clean_features_effects %>%
  select(all_of(c(model_features, "target")))

#Check for and handle any remaining NA values
model_data <- model_data %>% drop_na()

#Check for highly correlated predictors
cor_matrix <- cor(model_data %>% select(-target))
high_cors <- which(abs(cor_matrix) > 0.7 & abs(cor_matrix) < 1, arr.ind = TRUE)
high_cor_pairs <- data.frame(
  var1 = rownames(cor_matrix)[high_cors[,1]],
  var2 = colnames(cor_matrix)[high_cors[,2]],
  correlation = cor_matrix[high_cors]
)
high_cor_pairs <- high_cor_pairs %>% 
  filter(var1 < var2) %>% 
  arrange(desc(abs(correlation)))

print("Highly correlated feature pairs:")
print(high_cor_pairs)

#training (80%) and testing (20%)
set.seed(123)  # For reproducibility
train_index <- createDataPartition(model_data$target, p = 0.8, list = FALSE)
train_data <- model_data[train_index, ]
test_data <- model_data[-train_index, ]


table(train_data$target)

# LASSO Variable Selection
x_train <- as.matrix(train_data %>% select(-target))
y_train <- train_data$target

#Perform cross-validation to find optimal lambda
set.seed(123)
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial", nfolds = 10)

#Get the optimal lambda values
lambda_min <- cv_lasso$lambda.min  
lambda_1se <- cv_lasso$lambda.1se

cat("Lambda min:", lambda_min, "\n")
cat("Lambda 1se:", lambda_1se, "\n")

#Fit the LASSO model with the selected lambda
lasso_model <- glmnet(x_train, y_train, alpha = 1, family = "binomial", lambda = lambda_1se)


lasso_coefs <- coef(lasso_model)
selected_vars <- rownames(lasso_coefs)[which(lasso_coefs != 0)]
selected_vars <- selected_vars[selected_vars != "(Intercept)"]  # Remove intercept

cat("LASSO selected variables:", length(selected_vars), "out of", length(model_features), "\n")
print(selected_vars)

#Create a new data frame with only the selected variables
selected_data <- model_data %>% select(all_of(c(selected_vars, "target")))

#Split the selected data into training and testing sets
selected_train_data <- selected_data[train_index, ]
selected_test_data <- selected_data[-train_index, ]


# Logistic Regression Model
log_model <- glm(target ~ ., data = selected_train_data, family = binomial)

#logistic regression model
log_summary <- summary(log_model)
print(log_summary)

#Calculate feature importance based on absolute z-values
z_values <- abs(log_summary$coefficients[, "z value"][-1])  # Exclude intercept
log_importance <- data.frame(
  Variable = names(z_values),
  Importance = z_values
)
log_importance <- log_importance %>% arrange(desc(Importance))

#top important features
cat("\nMost important predictors:\n")
print(head(log_importance, 10))

#Make predictions on the test set
log_predictions <- predict(log_model, newdata = selected_test_data, type = "response")
log_pred_class <- ifelse(log_predictions > 0.5, 1, 0)

#logistic regression model
log_confusion <- confusionMatrix(factor(log_pred_class), factor(selected_test_data$target))
log_roc <- roc(selected_test_data$target, log_predictions)
log_auc <- auc(log_roc)

cat("\nLogistic Regression Model Performance:\n")
print(log_confusion)
cat("AUC:", log_auc, "\n")


```

Visualization for logistic regression with LASSO

```{r, warning = FALSE}
# Visualization of logistic regression variable importance
library(ggplot2)

# Create data frame with z-values for importance
log_importance <- data.frame(
  Variable = names(z_values),
  Importance = z_values
)

# Sort for better visualization
log_importance <- log_importance %>% arrange(desc(Importance))

# Create bar chart for top 8 variables
ggplot(head(log_importance, 8), aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = round(Importance, 2)), hjust = -0.2, size = 3) +
  coord_flip() +
  labs(title = "Variable Importance in Logistic Regression Model",
       subtitle = "Based on absolute z-values",
       x = "",
       y = "Absolute z-value") +
  theme_minimal() +
  ylim(0, max(log_importance$Importance) * 1.1)
```

random foresting for complete case

```{r, warning= FALSE}

library(tidyverse)
library(randomForest)  
library(caret)        
library(pROC)         

ufc_clean_features_effects <- read.csv("data/ufc_clean_features_effects.csv", stringsAsFactors = FALSE)

#list of model features
model_features <- c(

  "height_advantage", "reach_advantage", "age_advantage", "weight_advantage",
  
  "win_pct_advantage", "experience_diff",
  "R_current_win_streak", "B_current_win_streak", 
  "R_longest_win_streak", "B_longest_win_streak",
  
  "striking_accuracy_diff", "striking_defense_diff", 
  "takedown_accuracy_diff", "takedown_defense_diff", 
  "knockdown_diff", "submission_attempts_diff", "offense_defense_diff",
  
  "R_avg_SIG_STR_pct", "B_avg_SIG_STR_pct",
  "R_avg_TD_pct", "B_avg_TD_pct",
  "R_avg_SUB_ATT", "B_avg_SUB_ATT",
  "R_avg_KD", "B_avg_KD",
  
  "R_striking_to_grappling_ratio", "B_striking_to_grappling_ratio"
)

model_data <- ufc_clean_features_effects %>%
  select(all_of(c(model_features, "target")))

#Check for and handle any remaining NA values
model_data <- model_data %>% drop_na()


set.seed(123) 
train_index <- createDataPartition(model_data$target, p = 0.8, list = FALSE)
train_data <- model_data[train_index, ]
test_data <- model_data[-train_index, ]

train_data$target <- factor(train_data$target)
test_data$target <- factor(test_data$target)


#Train Random Forest Model
set.seed(123)
rf_model <- randomForest(
  target ~ ., 
  data = train_data,
  ntree = 500,          
  mtry = sqrt(ncol(train_data) - 1),  
  importance = TRUE     
)

print(rf_model)

#variable importance
var_importance <- importance(rf_model)
var_importance_df <- data.frame(
  Variable = rownames(var_importance),
  MeanDecreaseGini = var_importance[, "MeanDecreaseGini"]
) %>% arrange(desc(MeanDecreaseGini))

#top important features
cat("\nMost important predictors (Random Forest):\n")
print(head(var_importance_df, 10))

#predictions on the test set
rf_predictions <- predict(rf_model, newdata = test_data, type = "prob")[,2]
rf_pred_class <- predict(rf_model, newdata = test_data)

rf_confusion <- confusionMatrix(rf_pred_class, test_data$target)
rf_roc <- roc(as.numeric(test_data$target), rf_predictions)
rf_auc <- auc(rf_roc)

cat("\nRandom Forest Model Performance:\n")
print(rf_confusion)
cat("AUC:", rf_auc, "\n")

cat("\nClass-specific error rates:\n")
cat("Class 0 (Blue corner win) error rate:", rf_model$confusion[1, 3], "\n")
cat("Class 1 (Red corner win) error rate:", rf_model$confusion[2, 3], "\n")

cat("Overall OOB error rate:", rf_model$err.rate[nrow(rf_model$err.rate), "OOB"], "\n")


```

Visualization for Random Forest Classification:

```{r, warning = FALSE}
# Visualization for Random Forest variable importance
library(ggplot2)

# Variable importance plot (already in your code, but refined for presentation)
var_importance_df <- data.frame(
  Variable = rownames(importance(rf_model)),
  MeanDecreaseGini = importance(rf_model)[, "MeanDecreaseGini"]
) %>% arrange(desc(MeanDecreaseGini))

# Top 10 variables plot
ggplot(head(var_importance_df, 10), aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = "darkgreen") +
  coord_flip() +
  labs(title = "Top 10 Important Features in Random Forest Model",
       x = "",
       y = "Mean Decrease in Gini") +
  theme_minimal()

# Class-specific error rates visualization
error_rates <- data.frame(
  Class = c("Blue Corner (0)", "Red Corner (1)", "Overall"),
  Error_Rate = c(rf_model$confusion[1, 3], rf_model$confusion[2, 3], 
                rf_model$err.rate[nrow(rf_model$err.rate), "OOB"]) * 100
)

ggplot(error_rates, aes(x = Class, y = Error_Rate, fill = Class)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(Error_Rate, 1), "%")), vjust = -0.5, size = 4) +
  labs(title = "Random Forest Classification Error Rates",
       x = "",
       y = "Error Rate (%)") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set2") +
  theme(legend.position = "none") +
  ylim(0, max(error_rates$Error_Rate) * 1.1)
```

plots of variable importance:

```{r, warning= FALSE}

library(tidyverse)
library(gridExtra)
library(RColorBrewer)


log_importance <- data.frame(
  Variable = names(coef(log_model))[-1],  # Exclude intercept
  Importance = abs(summary(log_model)$coefficients[-1, "z value"]),
  Model = "Logistic Regression"
)
log_importance <- log_importance %>% arrange(desc(Importance))


rf_importance <- data.frame(
  Variable = rownames(importance(rf_model)),
  Importance = importance(rf_model)[, "MeanDecreaseGini"],
  Model = "Random Forest"
)
rf_importance <- rf_importance %>% arrange(desc(Importance))

#Logistic Regression Importance Plot
log_plot <- ggplot(log_importance %>% head(10), 
                   aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 10 Important Features (Logistic Regression)",
       x = "Feature",
       y = "Importance (|z-value|)") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold"),
        axis.text.y = element_text(size = 10))

#Importance Plot
rf_plot <- ggplot(rf_importance %>% head(10), 
                 aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "darkgreen") +
  coord_flip() +
  labs(title = "Top 10 Important Features (Random Forest)",
       x = "Feature",
       y = "Importance (Mean Decrease in Gini)") +
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold"),
        axis.text.y = element_text(size = 10))


#Normalize importance scores for fair comparison
log_importance$Importance_Normalized <- log_importance$Importance / max(log_importance$Importance)
rf_importance$Importance_Normalized <- rf_importance$Importance / max(rf_importance$Importance)

top_features <- unique(c(
  log_importance$Variable[1:10],
  rf_importance$Variable[1:10]
))

#data frame
importance_comparison <- data.frame(
  Variable = top_features,
  LogisticRegression = 0,
  RandomForest = 0
)

#normalized importance values
for (var in top_features) {
  if (var %in% log_importance$Variable) {
    importance_comparison$LogisticRegression[importance_comparison$Variable == var] <- 
      log_importance$Importance_Normalized[log_importance$Variable == var]
  }
  if (var %in% rf_importance$Variable) {
    importance_comparison$RandomForest[importance_comparison$Variable == var] <- 
      rf_importance$Importance_Normalized[rf_importance$Variable == var]
  }
}

importance_comparison_long <- importance_comparison %>%
  pivot_longer(cols = c(LogisticRegression, RandomForest),
               names_to = "Model",
               values_to = "Importance")

#comparison plot
comparison_plot <- ggplot(importance_comparison_long,
                         aes(x = reorder(Variable, Importance), 
                             y = Importance, 
                             fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Variable Importance Comparison",
       subtitle = "Normalized importance scores across both models",
       x = "Feature",
       y = "Normalized Importance") +
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(size = 14, face = "bold"),
        axis.text.y = element_text(size = 10))


grid.arrange(log_plot, rf_plot, ncol = 2)

print(comparison_plot)
```
