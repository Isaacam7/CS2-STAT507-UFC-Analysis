---
title: "ufc_model_builder"
author: 'Isaac Amouzou G#: G01307253'
date: "2025-04-29"
output: html_document
---

```{r}
rm(list = ls())
library(xgboost)
library(randomForest)
library(caret)
library(tidyverse)
library(lubridate)
library(parallel)
```


```{r}
na_omit <- read_csv("Data/na_omitted_data.csv") %>%
  mutate(across(where(is.numeric), ~ if(length(unique(.)) <= 3) factor(.) else .))

na_omit_numeric <- na_omit %>% 
  dplyr::select(where(is.numeric))

set.seed(507)
naomit.kmeans <- kmeans(na_omit_numeric, centers = 3)

na_omit$Fight_type <- as.factor(naomit.kmeans$cluster)
```
```{r}
na_omit <- na_omit %>% filter(Winner != "Draw") # Not many draws so keep it a binary problem
sapply(na_omit[ , sapply(na_omit, is.character)], function(x) length(unique(x))) # Check all characer columns
```
```{r}
colnames(na_omit[ , sapply(na_omit, is.factor)])
```

```{r}
na_omit <- na_omit %>% arrange(date)

test_info <- tail(na_omit,100) %>% dplyr::select(R_fighter, B_fighter, Referee, date)

na_omit <- na_omit %>% dplyr::select(-R_fighter,-B_fighter,-Referee,-location)

```

```{r}
X_naomit <- na_omit %>% dplyr::select(-Winner)
y <- if_else(na_omit$Winner == "Red", 1, 0)

categorical_vars <- c("weight_class", "B_Stance", "R_Stance", "Fight_type")

dummy_encoder <- dummyVars(~ ., data = X_naomit[, categorical_vars], fullRank = TRUE)
enconded_cat_vars <- predict(dummy_encoder, newdata = X_naomit[, categorical_vars])

X_naomit <- X_naomit %>% 
  dplyr::select(-all_of(categorical_vars)) %>%
  cbind(as.data.frame(enconded_cat_vars)) %>%
  mutate(
    doy = lubridate::yday(as.Date(X_naomit$date)),
    title_bout = if_else(na_omit$title_bout == TRUE, 1, 0)
    ) %>%
  dplyr::select(-date) 
```


```{r}
n <- nrow(X_naomit)
X_train_na <- X_naomit[1:(n - 100), ]
X_test_na  <- X_naomit[(n - 99):n, ]

y_train_na <- y[1:(n - 100)]
y_test_na  <- y[(n - 99):n]

sum(y) / n
```
62.73227% of fights end in a Red victor, this will also be the baseline to beat. (can I predict better than just saying red every time)

```{r}
X_train_na_numeric <- X_train_na %>%
     mutate(across(everything(), ~ as.numeric(as.character(.))))

X_test_na_numeric <- X_test_na %>%
     mutate(across(everything(), ~ as.numeric(as.character(.))))
```

```{r}
set.seed(507)

dtrain <- xgb.DMatrix(data = as.matrix(X_train_na_numeric), label = as.numeric(y_train_na))
dtest  <- xgb.DMatrix(data = as.matrix(X_test_na_numeric),  label = as.numeric(y_test_na))


params <- list(
  booster = "gbtree",
  objective = "binary:logistic",     
  eval_metric = "error",              
  eta = 0.1,                          
  max_depth = 3,
  gamma = 0.1,
  min_child_weight = 1,
  subsample = 0.8,
  colsample_bytree = 0.5,
  scale_pos_weight = 0.61
)

xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 200,
  verbose = 0
)


pred_probs <- predict(xgb_model, dtest)

pred_labels <- ifelse(pred_probs > 0.5, 1, 0)

accuracy <- sum(pred_labels == y_test_na) / length(y_test_na)
cat("Accuracy: ", accuracy, "\n")
```

Just meets baseline. Time for hyperparameter tuning.

```{r}

nrounds_values <- c(50,150,200,250) 
max_depth_values <- c(3, 6, 9)
eta_values <- c(0.01, 0.05, 0.1)
min_child_weight_values <- c(1, 3, 5)
subsample_values <- c(0.4, 0.8)
colsample_bytree_values <- c(0.6, 0.8, 1.0)
gamma_values <- c(0, 0.25, 1)
scale_pos_weight <- c(0.5,0.6,0.7,1)

param_grid <- expand.grid(
  nrounds = nrounds_values,
  max_depth = max_depth_values,
  eta = eta_values,
  min_child_weight = min_child_weight_values,
  subsample = subsample_values,
  colsample_bytree = colsample_bytree_values,
  gamma = gamma_values,
  scale_pos_weight = scale_pos_weight
)
```

```{r}
train_xgb <- function(params) {
  nrounds <- params[1]
  max_depth <- params[2]
  eta <- params[3]
  min_child_weight <- params[4]
  subsample <- params[5]
  colsample_bytree <- params[6]
  gamma <- params[7]
  spw <- params[8]

  dtrain <- xgb.DMatrix(data = as.matrix(X_train_na_numeric), label = y_train_na)
  dtest  <- xgb.DMatrix(data = as.matrix(X_test_na_numeric), label = y_test_na)

  param_list <- list(
    booster = "gbtree",
    objective = "binary:logistic",
    eval_metric = "error",
    max_depth = max_depth,
    eta = eta,
    min_child_weight = min_child_weight,
    subsample = subsample,
    colsample_bytree = colsample_bytree,
    gamma = gamma,
    scale_pos_weight = spw
  )

  model <- xgb.train(
    params = param_list,
    data = dtrain,
    nrounds = nrounds,
    verbose = 0
  )

  preds_prob <- predict(model, dtest)
  preds <- ifelse(preds_prob > 0.5, 1, 0)

  accuracy <- mean(preds == y_test_na)

  return(data.frame(
    nrounds = nrounds, max_depth = max_depth, eta = eta, gamma = gamma,
    min_child_weight = min_child_weight, subsample = subsample,
    colsample_bytree = colsample_bytree, scale_pos_weight = spw,
    Accuracy = accuracy
  ))
}
```

```{r}
(num_cores <- detectCores(logical = F) - 1)
```
The code below is LONG (takes 2.7 hours)
```{r}
(starttime <- Sys.time())

cl <- makeCluster(num_cores)

clusterExport(cl, varlist = c("train_xgb", "param_grid", 
                              "X_train_na_numeric", "X_test_na_numeric",
                              "y_train_na", "y_test_na"))
clusterEvalQ(cl, {
  library(xgboost)
  library(caret)
})

results_list <- parLapply(cl, 1:nrow(param_grid), function(i) {
  train_xgb(as.numeric(param_grid[i, ]))
})

stopCluster(cl)

results_xgb <- bind_rows(results_list)

(endttime <- Sys.time())
```


```{r}
endttime - starttime
```

```{r}
#write.csv(results_xgb %>% arrange(-Accuracy), "Data/na_omit_xgb_results.csv", row.names = FALSE)
```

